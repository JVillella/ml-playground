{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wiki2vec\n",
    "\n",
    "Jupyter notebook for creating a [Word2vec](https://en.wikipedia.org/wiki/Word2vec) model from a Wikipedia dump. This model file can then be read into [gensim's Word2Vec class](https://radimrehurek.com/gensim/models/word2vec.html). Feel free to edit this script as you see fit.\n",
    "\n",
    "### Dependencies\n",
    "- Python 3\n",
    "- Jupyter\n",
    "- Gensim\n",
    "\n",
    "### Steps\n",
    "- Download a Wikipedia dump by visiting\n",
    "\n",
    "```\n",
    "https://dumps.wikimedia.org/<locale>wiki/latest/<locale>wiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "E.x. https://dumps.wikimedia.org/itwiki/latest/itwiki-latest-pages-articles.xml.bz2\n",
    "```\n",
    "- Once downloaded assign the following paths below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WIKIPEDIA_DUMP_PATH = './data/wiki-corpuses/enwiki-latest-pages-articles.xml.bz2'\n",
    "\n",
    "# Choose a path that the word2vec model should be saved to\n",
    "# (during training), and read from afterwards.\n",
    "WIKIPEDIA_W2V_PATH = './data/enwiki.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train Word2vec on Wikipedia dump\n",
    "\n",
    "Here is where we train the word2vec model on the given Wikipedia dump. Specifically we,\n",
    "\n",
    "1. Read given Wikipedia dump with gensim\n",
    "2. Write to temporary text file (will get deleted)\n",
    "3. Train word2vec model\n",
    "4. Save word2vec model\n",
    "\n",
    "*NB: 1 Wikipedia article is fed into word2vec as a single sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JVillella/Development/ml-playground/wiki2vec/venv/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tempfile\n",
    "import multiprocessing\n",
    "import logging\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def write_wiki_corpus(wiki, output_file):\n",
    "    \"\"\"Write a WikiCorpus as plain text to file.\"\"\"\n",
    "    \n",
    "    i = 0\n",
    "    for text in wiki.get_texts():\n",
    "        text_output_file.write(b' '.join(text) + b'\\n')\n",
    "        i = i + 1\n",
    "        if (i % 10000 == 0):\n",
    "            print('\\rSaved %d articles' % i, end='', flush=True)\n",
    "            \n",
    "    print('\\rFinished saving %d articles' % i, end='', flush=True)\n",
    "    \n",
    "def build_trained_model(text_file):\n",
    "    \"\"\"Reads text file and returns a trained model.\"\"\"\n",
    "    \n",
    "    sentences = LineSentence(text_file)\n",
    "    model = Word2Vec(sentences, size=400, window=5, min_count=5,\n",
    "                     workers=multiprocessing.cpu_count())\n",
    "\n",
    "    # Trim unneeded model memory to reduce RAM usage\n",
    "    model.init_sims(replace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4210000 articles"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 01:15:54,111 : INFO : finished iterating over Wikipedia corpus of 4211808 documents with 2303160832 positions (total 17246072 articles, 2365552029 positions before pruning articles shorter than 50 words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Finished saving 4211808 articles"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "logging_format = '%(asctime)s : %(levelname)s : %(message)s'\n",
    "logging.basicConfig(format=logging_format, level=logging.INFO)\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.txt') as text_output_file:\n",
    "    # Create wiki corpus, and save text to temp file\n",
    "    wiki_corpus = WikiCorpus(WIKIPEDIA_DUMP_PATH, lemmatize=False, dictionary={})\n",
    "    write_wiki_corpus(wiki_corpus, text_output_file)\n",
    "    del wiki_corpus\n",
    "\n",
    "    # Train model on wiki corpus\n",
    "    model = build_trained_model(text_output_file)    \n",
    "    model.save(WIKIPEDIA_W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Demo word2vec\n",
    "\n",
    "Read in the saved word2vec model and perform some basic analysis on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 04:58:25,818 : INFO : loading Word2Vec object from ./data/enwiki.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1 µs, total: 2 µs\n",
      "Wall time: 17.9 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 04:58:33,557 : INFO : loading syn1neg from ./data/enwiki.model.syn1neg.npy with mmap=None\n",
      "2017-02-21 04:58:39,667 : INFO : loading syn0 from ./data/enwiki.model.syn0.npy with mmap=None\n",
      "2017-02-21 04:58:58,673 : INFO : setting ignored attribute cum_table to None\n",
      "2017-02-21 04:58:58,674 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-02-21 04:58:58,675 : INFO : loaded ./data/enwiki.model\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "model = Word2Vec.load(WIKIPEDIA_W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sample: ['nettlestead', 'maaples', 'giniel', 'zahivi', 'mievs']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(model.vocab.keys())\n",
    "print('Vocabulary sample:', vocab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-21 04:59:17,888 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to: parasphenoid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('quadratojugal', 0.7142307162284851),\n",
       " ('basisphenoid', 0.713142991065979),\n",
       " ('basioccipital', 0.7118856310844421),\n",
       " ('squamosal', 0.697265625),\n",
       " ('coracoid', 0.6788373589515686),\n",
       " ('premaxillae', 0.6749427914619446),\n",
       " ('postorbital', 0.6736751794815063),\n",
       " ('uncinate', 0.6717867851257324),\n",
       " ('basipterygoid', 0.6691710948944092),\n",
       " ('pterygoid', 0.6659449338912964)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = random.choice(vocab)\n",
    "\n",
    "print('Similar words to:', word)\n",
    "model.most_similar(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(xuanhe, vatnsskarð) = 0.070190\n"
     ]
    }
   ],
   "source": [
    "word1 = random.choice(vocab)\n",
    "word2 = random.choice(vocab)\n",
    "print('similarity(%s, %s) = %f' % (word1, word2, model.similarity(word1, word2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
